{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sorok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sorok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sorok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sorok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sorok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this eBook or online at\\nwww.gutenberg.org. If you are not located in the United States, you\\nwill have to check the laws of the country where you are located'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file\n",
    "\n",
    "filename = 'alice_in_wonderland.txt'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "    alice_in_wonderland = f.read()\n",
    "alice_in_wonderland[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be w'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the text\n",
    "\n",
    "textfile = alice_in_wonderland.split('CHAPTER I.')[2]\n",
    "textfile = textfile.split('THE END')[0]\n",
    "textfile[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess a text\n",
    "\n",
    "def preprocess_textfile(input_text):\n",
    "    formatted_text = input_text.lower()\n",
    "    formatted_text = re.sub(r\"won\\'t\", \"will not\", formatted_text)\n",
    "    formatted_text = re.sub(r\"can\\'t\", \"can not\", formatted_text)\n",
    "    formatted_text = re.sub(r\"n\\'t\", \" not\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'re\", \" are\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'s\", \" is\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'d\", \" would\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'ll\", \" will\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'t\", \" not\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'ve\", \" have\", formatted_text)\n",
    "    formatted_text = re.sub(r\"\\'m\", \" am\", formatted_text)\n",
    "    formatted_text = re.sub(r\"[^\\w\\s]\", \"\", formatted_text)\n",
    "    formatted_text = re.sub(\"_\", \" \", formatted_text)\n",
    "    text_tokens = formatted_text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sw_tokens = [token for token in text_tokens if not token in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_tokens = [lemmatizer.lemmatize(token) for token in sw_tokens]\n",
    "    lemm_tokens = [lemmatizer.lemmatize(token, \"v\") for token in sw_tokens]    # For verbs\n",
    "    formatted_text = \" \".join(lemm_tokens)\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 MOST IMPORTANT WORDS FROM EACH CHAPTER.\n",
      "CHAPTER  1 :\n",
      "think eat say little bat fall key try wonder happen \n",
      "\n",
      "CHAPTER  2 :\n",
      "mouse say ill pool little swim think cat dear fan \n",
      "\n",
      "CHAPTER  3 :\n",
      "say mouse dodo prize bird lory ill dry know thimble \n",
      "\n",
      "CHAPTER  4 :\n",
      "grow little window rabbit say run fan puppy hear come \n",
      "\n",
      "CHAPTER  5 :\n",
      "say caterpillar pigeon serpent egg youth bite try think size \n",
      "\n",
      "CHAPTER  6 :\n",
      "say cat footman baby mad duchess sneeze pig grin think \n",
      "\n",
      "CHAPTER  7 :\n",
      "say hatter dormouse march hare twinkle draw remark time know \n",
      "\n",
      "CHAPTER  8 :\n",
      "queen say soldier gardeners look king hedgehog cat head executioner \n",
      "\n",
      "CHAPTER  9 :\n",
      "say turtle mock gryphon duchess moral queen think sigh remark \n",
      "\n",
      "CHAPTER  10 :\n",
      "turtle mock gryphon say dance beautiful soup join repeat lobster \n",
      "\n",
      "CHAPTER  11 :\n",
      "king hatter say court dormouse witness begin queen officer look \n",
      "\n",
      "CHAPTER  12 :\n",
      "say king jury write dream queen sister mean rabbit jurymen \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Find 10 most important words from each chapter\n",
    "\n",
    "all_chapters = textfile.split('CHAPTER ')\n",
    "tf_idf = TfidfVectorizer(stop_words='english')\n",
    "tf_idf.fit_transform(all_chapters)\n",
    "print(\"10 MOST IMPORTANT WORDS FROM EACH CHAPTER.\")\n",
    "chapter_num = 1\n",
    "for chapter in all_chapters:\n",
    "    clear_chapter = preprocess_textfile(chapter)\n",
    "    clear_chapter = re.sub(r'alice', '', clear_chapter)  # No Alice for this task\n",
    "    response = tf_idf.transform([clear_chapter])\n",
    "    all_words = np.array(tf_idf.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "    top_ten = all_words[tfidf_sorting][:10]\n",
    "    print(\"CHAPTER \", chapter_num, \":\")\n",
    "    for i in top_ten:\n",
    "        print(i, end=\" \")\n",
    "    print(\"\\n\")\n",
    "    chapter_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 MOST USED VERBS IN SENTENCES WITH ALICE:\n",
      "say think know run come make begin happen look tell "
     ]
    }
   ],
   "source": [
    "# Task 4: Find the Top 10 most used verbs in sentences with Alice\n",
    "\n",
    "sents = sent_tokenize(textfile)\n",
    "alice_sents = []\n",
    "verbs = ''\n",
    "for sentence in sents:\n",
    "    clearSentence = preprocess_textfile(sentence)\n",
    "    clearSentence = re.sub(r'king', '', clearSentence)  # It's a noun, but considered as verb\n",
    "    if 'alice' in clearSentence:\n",
    "        alice_sents.append(clearSentence)\n",
    "    else:\n",
    "        continue\n",
    "for sentence in alice_sents:\n",
    "    s = nltk.pos_tag(sentence.split())\n",
    "    for w in s:\n",
    "        if 'VB' in w[1]:\n",
    "            verbs += w[0] + ' '\n",
    "        else:\n",
    "            continue\n",
    "response = tf_idf.transform([verbs])\n",
    "feature_array = np.array(tf_idf.get_feature_names())\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "print(\"10 MOST USED VERBS IN SENTENCES WITH ALICE:\")\n",
    "for i in feature_array[tfidf_sorting][:10]:\n",
    "        print(i, end=\" \")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "389de99e30140ebdcfbe07e8f44a1e63448e4477ae86445d6474d742bd4a815f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
